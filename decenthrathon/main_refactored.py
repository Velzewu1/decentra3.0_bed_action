#!/usr/bin/env python3
"""
Main Entry Point
CLI Interface + Orchestration
"""

import argparse
import sys
import time
import logging
import os
from pathlib import Path
import numpy as np

# Local imports
from config import get_config
from utils.helpers import (
    setup_logging, load_transaction_data, validate_dataframe_schema, 
    validate_features_dataframe, clean_features_dataframe, 
    set_random_state, print_dataframe_info, save_dataframe
)
from core.data_processing import process_transaction_data
from core.clustering import perform_clustering
from reporting.reports import generate_comprehensive_report

def setup_cli_parser() -> argparse.ArgumentParser:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    parser = argparse.ArgumentParser(
        description="üèÜ Ultra-Optimized Customer Segmentation Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --data transactions.csv                    # Basic run
  %(prog)s --data transactions.csv --random-state 123 # Fixed seed  
  %(prog)s --data transactions.csv --log-level DEBUG  # Detailed logs
  %(prog)s --data transactions.csv --no-gpt4          # Skip GPT-4 insights
        """
    )
    
    # Data arguments
    parser.add_argument(
        '--data', 
        type=str, 
        default='transactions.csv',
        help='Path to transactions CSV file (default: transactions.csv)'
    )
    
    # Reproducibility
    parser.add_argument(
        '--random-state', 
        type=int, 
        default=42,
        help='Random state for reproducibility (default: 42)'
    )
    
    # Logging
    parser.add_argument(
        '--log-level', 
        type=str, 
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='Logging level (default: INFO)'
    )
    
    parser.add_argument(
        '--log-file', 
        type=str,
        help='Log file path (optional)'
    )
    
    # Features
    parser.add_argument(
        '--no-gpt4', 
        action='store_true',
        help='Skip GPT-4 insights generation'
    )
    
    parser.add_argument(
        '--validate-only', 
        action='store_true',
        help='Only validate data and exit'
    )
    
    return parser

def validate_input_args(args) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    logger = logging.getLogger(__name__)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    if not Path(args.data).exists():
        logger.error(f"‚ùå Data file not found: {args.data}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º random state
    if args.random_state < 0:
        logger.error(f"‚ùå Random state must be non-negative: {args.random_state}")
        return False
    
    logger.info("‚úÖ Input arguments validated")
    return True

def print_pipeline_header():
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ pipeline"""
    print("=" * 80)
    print("üèÜ ULTRA-OPTIMIZED CUSTOMER SEGMENTATION PIPELINE")
    print("=" * 80)
    print("üìä Advanced HDBSCAN with Business Intelligence")
    print("üîß Feature Engineering + Preprocessing + Ultra-Optimization")
    print("üß† GPT-4 Insights + Comprehensive Reporting")
    print("=" * 80)

def print_pipeline_summary(report: dict, execution_time: float):
    """–ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ pipeline"""
    logger = logging.getLogger(__name__)
    
    print("\n" + "=" * 80)
    print("üèÜ PIPELINE EXECUTION SUMMARY")
    print("=" * 80)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    metrics = report['quality_metrics']
    logger.info(f"üéØ Algorithm: {report['algorithm']}")
    logger.info(f"üéØ Clusters Found: {metrics['n_clusters']}")
    logger.info(f"üîá Noise Ratio: {metrics['noise_ratio']:.1%}")
    logger.info(f"üìà Silhouette Score: {metrics.get('silhouette_score', 'N/A')}")
    logger.info(f"üìä Davies-Bouldin: {metrics.get('davies_bouldin', 'N/A')}")
    logger.info(f"‚ö° Calinski-Harabasz: {metrics.get('calinski_harabasz', 'N/A')}")
    
    # –ë–∏–∑–Ω–µ—Å –∏–Ω—Å–∞–π—Ç—ã
    insights = report['insights']
    logger.info(f"üß† Business Segments: {len(insights)}")
    
    # –§–∞–π–ª—ã
    if report['export_success']:
        logger.info("üìÅ Generated Files:")
        for name, path in report['files_created'].items():
            logger.info(f"   üìã {path}")
    
    # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    logger.info(f"‚è±Ô∏è Total Execution Time: {execution_time:.1f} seconds")
    
    print("=" * 80)
    print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
    print("=" * 80)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è pipeline"""
    start_time = time.time()
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = setup_cli_parser()
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging(level=args.log_level, log_file=args.log_file)
    logger = logging.getLogger(__name__)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    print_pipeline_header()
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if not validate_input_args(args):
            sys.exit(1)
        
        # –§–∏–∫—Å–∏—Ä—É–µ–º random state
        set_random_state(args.random_state)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = get_config()
        config.print_config_summary()
        
        # ========================================================================
        # STAGE 1: DATA LOADING & VALIDATION
        # ========================================================================
        logger.info("üöÄ STAGE 1: DATA LOADING & VALIDATION")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = load_transaction_data(args.data)
        print_dataframe_info(df, "Transaction Data")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å—Ö–µ–º—É
        expected_columns = config.FEATURE_CONFIG['expected_columns']
        if not validate_dataframe_schema(df, expected_columns):
            logger.error("‚ùå Data validation failed")
            sys.exit(1)
        
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è - –≤—ã—Ö–æ–¥–∏–º
        if args.validate_only:
            logger.info("‚úÖ Data validation completed successfully")
            sys.exit(0)
        
        # ========================================================================
        # STAGE 2: FEATURE ENGINEERING & PREPROCESSING
        # ========================================================================
        logger.info("üöÄ STAGE 2: FEATURE ENGINEERING & PREPROCESSING")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        features_df, ml_features_processed, preprocessing_pipeline = process_transaction_data(df)
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –æ—á–∏—â–∞–µ–º —Ñ–∏—á–∏
        if not validate_features_dataframe(features_df):
            logger.error("‚ùå Features validation failed")
            sys.exit(1)
        
        features_df = clean_features_dataframe(features_df)
        print_dataframe_info(features_df, "Customer Features")
        
        # ========================================================================
        # STAGE 3: ULTRA-OPTIMIZED CLUSTERING
        # ========================================================================
        logger.info("üöÄ STAGE 3: ULTRA-OPTIMIZED CLUSTERING")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
        labels, clusterer, clustering_results = perform_clustering(
            ml_features_processed, 
            config.CLUSTERING_PARAMS
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        features_df['segment'] = labels
        
        # ========================================================================
        # STAGE 4: COMPREHENSIVE REPORTING
        # ========================================================================
        logger.info("üöÄ STAGE 4: COMPREHENSIVE REPORTING")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º results –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
        algorithm_name = clustering_results.get('gmm_best', clustering_results.get('kmeans_best', clustering_results.get('hdbscan_best', {})))
        algorithm_key = f"{algorithm_name.get('params', {}).get('algorithm', 'unknown')}_balanced"
        
        formatted_results = {
            algorithm_key: {
                'labels': labels,
                'clusterer': clusterer,
                'params': algorithm_name.get('params', {}),
                'metrics': algorithm_name.get('metrics', {}),
                'algorithm_name': f"Balanced {algorithm_name.get('params', {}).get('algorithm', 'Unknown').upper()}"
            }
        }
        
        final_report = generate_comprehensive_report(
            features_df, labels, ml_features_processed, formatted_results
        )
        
        # ========================================================================
        # STAGE 5: PIPELINE SUMMARY
        # ========================================================================
        execution_time = time.time() - start_time
        print_pipeline_summary(final_report, execution_time)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        output_files = [
            'customer_segments_refactored.csv',
            'customer_segments_refactored.parquet'
        ]
        
        for filename in output_files:
            save_dataframe(features_df, filename)
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        total_time = time.time() - start_time
        print(f"\n‚è±Ô∏è  –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        print(f"\nüéâ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"üìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(set(labels))}")
        print(f"üë• –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(features_df):,}")
        print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.getcwd()}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        unique_labels = np.unique(labels)
        cluster_sizes = [np.sum(labels == label) for label in unique_labels]
        max_size = max(cluster_sizes)
        min_size = min(cluster_sizes)
        balance_ratio = min_size / max_size
        largest_pct = max_size / len(labels) * 100
        
        print(f"\nüìà –ö–ê–ß–ï–°–¢–í–û –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò:")
        print(f"   –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {sorted(cluster_sizes, reverse=True)}")
        print(f"   –ë–∞–ª–∞–Ω—Å (–º–∏–Ω/–º–∞–∫—Å): {balance_ratio:.3f}")
        print(f"   –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä: {largest_pct:.1f}%")
        
        if largest_pct < 50:
            print(f"   ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å!")
        elif largest_pct < 70:
            print(f"   ‚ö†Ô∏è  –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")
        else:
            print(f"   ‚ùå –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Pipeline interrupted by user")
        return 1
        
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {e}")
        logger.debug("Exception details:", exc_info=True)
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 