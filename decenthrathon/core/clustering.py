#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
"""

import numpy as np
import pandas as pd
import hdbscan
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import logging
from typing import Dict, Tuple, Any
import time

logger = logging.getLogger(__name__)

def evaluate_clustering_balance(labels: np.ndarray) -> Dict[str, float]:
    """–û—Ü–µ–Ω–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    unique_labels = np.unique(labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
    noise_ratio = np.sum(labels == -1) / len(labels) if -1 in labels else 0.0
    
    # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∏—Å–∫–ª—é—á–∞—è —à—É–º)
    cluster_sizes = []
    for label in unique_labels:
        if label != -1:
            size = np.sum(labels == label)
            cluster_sizes.append(size)
    
    if len(cluster_sizes) == 0:
        return {
            'n_clusters': 0,
            'noise_ratio': noise_ratio,
            'balance_ratio': 0.0,
            'largest_cluster_pct': 0.0,
            'silhouette': None
        }
    
    max_size = max(cluster_sizes)
    min_size = min(cluster_sizes)
    balance_ratio = min_size / max_size if max_size > 0 else 0
    largest_cluster_pct = max_size / len(labels) * 100
    
    return {
        'n_clusters': n_clusters,
        'noise_ratio': noise_ratio,
        'balance_ratio': balance_ratio,
        'largest_cluster_pct': largest_cluster_pct,
        'cluster_sizes': sorted(cluster_sizes, reverse=True)
    }

def calculate_silhouette_safe(X: np.ndarray, labels: np.ndarray) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç silhouette score"""
    try:
        # –£–±–∏—Ä–∞–µ–º —à—É–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞
        non_noise = labels != -1
        if np.sum(non_noise) < 2:
            return None
            
        unique_labels = np.unique(labels[non_noise])
        if len(unique_labels) < 2:
            return None
            
        return silhouette_score(X[non_noise], labels[non_noise])
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ silhouette score: {e}")
        return None

def run_gmm_clustering(X: np.ndarray, params: Dict) -> Dict[str, Any]:
    """Gaussian Mixture Model –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    logger.info("üîÆ –ó–∞–ø—É—Å–∫ GMM –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    best_result = None
    best_score = -1
    results = []
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    for n_components in params.get('gmm_n_components_range', [3, 4, 5]):
        for covariance_type in params.get('gmm_covariance_type', ['full']):
            try:
                start_time = time.time()
                
                gmm = GaussianMixture(
                    n_components=n_components,
                    covariance_type=covariance_type,
                    init_params=params.get('gmm_init_params', 'kmeans'),
                    max_iter=params.get('gmm_max_iter', 100),
                    tol=params.get('gmm_tol', 1e-3),
                    random_state=params.get('gmm_random_state', 42)
                )
                
                labels = gmm.fit_predict(X)
                fit_time = time.time() - start_time
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                balance_metrics = evaluate_clustering_balance(labels)
                silhouette = calculate_silhouette_safe(X, labels)
                
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score: –±–∞–ª–∞–Ω—Å (60%) + silhouette (40%)
                if silhouette is not None:
                    combined_score = balance_metrics['balance_ratio'] * 0.6 + silhouette * 0.4
                else:
                    combined_score = balance_metrics['balance_ratio'] * 0.6
                
                result = {
                    'labels': labels,
                    'clusterer': gmm,
                    'params': {
                        'algorithm': 'gmm',
                        'n_components': n_components,
                        'covariance_type': covariance_type
                    },
                    'metrics': {
                        **balance_metrics,
                        'silhouette': silhouette,
                        'combined_score': combined_score,
                        'fit_time': fit_time
                    }
                }
                
                results.append(result)
                
                silhouette_str = f"{silhouette:.3f}" if silhouette is not None else "N/A"
                logger.info(f"  n_components={n_components}, cov={covariance_type}: "
                          f"{balance_metrics['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
                          f"–±–∞–ª–∞–Ω—Å={balance_metrics['balance_ratio']:.3f}, "
                          f"–º–∞–∫—Å={balance_metrics['largest_cluster_pct']:.1f}%, "
                          f"silhouette={silhouette_str}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if combined_score > best_score:
                    best_score = combined_score
                    best_result = result
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GMM (n={n_components}, cov={covariance_type}): {e}")
    
    if best_result is None:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å GMM –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
    
    logger.info(f"‚úÖ –õ—É—á—à–∏–π GMM —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "
              f"{best_result['metrics']['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
              f"–±–∞–ª–∞–Ω—Å={best_result['metrics']['balance_ratio']:.3f}, "
              f"score={best_score:.3f}")
    
    return {'gmm_best': best_result, 'gmm_all_results': results}

def run_kmeans_clustering(X: np.ndarray, params: Dict) -> Dict[str, Any]:
    """K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    logger.info("üìä –ó–∞–ø—É—Å–∫ K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    best_result = None
    best_score = -1
    results = []
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    for n_clusters in params.get('kmeans_n_clusters_range', [3, 4, 5, 6, 7]):
        try:
            start_time = time.time()
            
            kmeans = KMeans(
                n_clusters=n_clusters,
                init=params.get('kmeans_init', 'k-means++'),
                n_init=params.get('kmeans_n_init', 10),
                max_iter=params.get('kmeans_max_iter', 300),
                random_state=params.get('kmeans_random_state', 42)
            )
            
            labels = kmeans.fit_predict(X)
            fit_time = time.time() - start_time
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            balance_metrics = evaluate_clustering_balance(labels)
            silhouette = calculate_silhouette_safe(X, labels)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            if silhouette is not None:
                combined_score = balance_metrics['balance_ratio'] * 0.6 + silhouette * 0.4
            else:
                combined_score = balance_metrics['balance_ratio'] * 0.6
            
            result = {
                'labels': labels,
                'clusterer': kmeans,
                'params': {
                    'algorithm': 'kmeans',
                    'n_clusters': n_clusters
                },
                'metrics': {
                    **balance_metrics,
                    'silhouette': silhouette,
                    'combined_score': combined_score,
                    'fit_time': fit_time
                }
            }
            
            results.append(result)
            
            silhouette_str = f"{silhouette:.3f}" if silhouette is not None else "N/A"
            logger.info(f"  k={n_clusters}: "
                      f"{balance_metrics['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
                      f"–±–∞–ª–∞–Ω—Å={balance_metrics['balance_ratio']:.3f}, "
                      f"–º–∞–∫—Å={balance_metrics['largest_cluster_pct']:.1f}%, "
                      f"silhouette={silhouette_str}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if combined_score > best_score:
                best_score = combined_score
                best_result = result
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ K-Means (k={n_clusters}): {e}")
    
    if best_result is None:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
    
    logger.info(f"‚úÖ –õ—É—á—à–∏–π K-Means —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "
              f"{best_result['metrics']['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
              f"–±–∞–ª–∞–Ω—Å={best_result['metrics']['balance_ratio']:.3f}")
    
    return {'kmeans_best': best_result, 'kmeans_all_results': results}

def run_hdbscan_clustering(X: np.ndarray, params: Dict) -> Dict[str, Any]:
    """HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)"""
    logger.info("üî¨ –ó–∞–ø—É—Å–∫ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    best_result = None
    best_score = -1
    results = []
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    min_cluster_sizes = params.get('min_cluster_size_range', [5, 10])[:2]
    min_samples_list = params.get('min_samples_range', [3, 5])[:2]
    
    for min_cluster_size in min_cluster_sizes:
        for min_samples in min_samples_list:
            try:
                start_time = time.time()
                
                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples,
                    cluster_selection_epsilon=0.0,
                    cluster_selection_method='eom',
                    metric='euclidean',
                    n_jobs=params.get('n_jobs', -1),
                    core_dist_n_jobs=params.get('core_dist_n_jobs', -1)
                )
                
                labels = clusterer.fit_predict(X)
                fit_time = time.time() - start_time
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                balance_metrics = evaluate_clustering_balance(labels)
                silhouette = calculate_silhouette_safe(X, labels)
                
                # –î–ª—è HDBSCAN –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç silhouette
                if silhouette is not None:
                    combined_score = silhouette * 0.7 + balance_metrics['balance_ratio'] * 0.3
                else:
                    combined_score = balance_metrics['balance_ratio'] * 0.3
                
                result = {
                    'labels': labels,
                    'clusterer': clusterer,
                    'params': {
                        'algorithm': 'hdbscan',
                        'min_cluster_size': min_cluster_size,
                        'min_samples': min_samples
                    },
                    'metrics': {
                        **balance_metrics,
                        'silhouette': silhouette,
                        'combined_score': combined_score,
                        'fit_time': fit_time
                    }
                }
                
                results.append(result)
                
                silhouette_str = f"{silhouette:.3f}" if silhouette is not None else "N/A"
                logger.info(f"  min_size={min_cluster_size}, min_samples={min_samples}: "
                          f"{balance_metrics['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
                          f"—à—É–º={balance_metrics['noise_ratio']:.1%}, "
                          f"silhouette={silhouette_str}")
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_result = result
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ HDBSCAN: {e}")
    
    if best_result is None:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
    
    return {'hdbscan_best': best_result, 'hdbscan_all_results': results}

def perform_clustering(X: np.ndarray, params: Dict) -> Tuple[np.ndarray, Any, Dict]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    
    Returns:
        Tuple[labels, clusterer, results_dict]
    """
    logger.info("üéØ –ó–∞–ø—É—Å–∫ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    algorithm = params.get('algorithm', 'gmm').lower()
    
    if algorithm == 'gmm':
        results = run_gmm_clustering(X, params)
        best_result = results['gmm_best']
    elif algorithm == 'kmeans':
        results = run_kmeans_clustering(X, params)
        best_result = results['kmeans_best']
    elif algorithm == 'hdbscan':
        results = run_hdbscan_clustering(X, params)
        best_result = results['hdbscan_best']
    else:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤...")
        
        all_results = {}
        best_overall = None
        best_overall_score = -1
        
        # GMM
        try:
            gmm_results = run_gmm_clustering(X, params)
            all_results.update(gmm_results)
            if gmm_results['gmm_best']['metrics']['combined_score'] > best_overall_score:
                best_overall_score = gmm_results['gmm_best']['metrics']['combined_score']
                best_overall = gmm_results['gmm_best']
        except Exception as e:
            logger.error(f"GMM failed: {e}")
        
        # K-Means
        try:
            kmeans_results = run_kmeans_clustering(X, params)
            all_results.update(kmeans_results)
            if kmeans_results['kmeans_best']['metrics']['combined_score'] > best_overall_score:
                best_overall_score = kmeans_results['kmeans_best']['metrics']['combined_score']
                best_overall = kmeans_results['kmeans_best']
        except Exception as e:
            logger.error(f"K-Means failed: {e}")
        
        # HDBSCAN
        try:
            hdbscan_results = run_hdbscan_clustering(X, params)
            all_results.update(hdbscan_results)
            if hdbscan_results['hdbscan_best']['metrics']['combined_score'] > best_overall_score:
                best_overall_score = hdbscan_results['hdbscan_best']['metrics']['combined_score']
                best_overall = hdbscan_results['hdbscan_best']
        except Exception as e:
            logger.error(f"HDBSCAN failed: {e}")
        
        if best_overall is None:
            raise ValueError("–í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π")
        
        results = all_results
        best_result = best_overall
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    metrics = best_result['metrics']
    silhouette_str = f"{metrics['silhouette']:.3f}" if metrics['silhouette'] is not None else "N/A"
    logger.info(f"üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
    logger.info(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {best_result['params']['algorithm'].upper()}")
    logger.info(f"   –ö–ª–∞—Å—Ç–µ—Ä—ã: {metrics['n_clusters']}")
    logger.info(f"   –†–∞–∑–º–µ—Ä—ã: {metrics.get('cluster_sizes', 'N/A')}")
    logger.info(f"   –ë–∞–ª–∞–Ω—Å: {metrics['balance_ratio']:.3f}")
    logger.info(f"   –°–∞–º—ã–π –±–æ–ª—å—à–æ–π: {metrics['largest_cluster_pct']:.1f}%")
    logger.info(f"   Silhouette: {silhouette_str}")
    logger.info(f"   –í—Ä–µ–º—è: {metrics['fit_time']:.1f}—Å")
    
    return best_result['labels'], best_result['clusterer'], results 