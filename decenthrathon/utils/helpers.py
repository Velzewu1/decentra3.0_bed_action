#!/usr/bin/env python3
"""
Helper Utilities Module
–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∞–ª–∏–¥–∞—Ü–∏—è
"""

import pandas as pd
import numpy as np
import logging
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any
import warnings

def setup_logging(level: str = 'INFO', log_file: Optional[str] = None) -> logging.Logger:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        level: –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (DEBUG, INFO, WARNING, ERROR)
        log_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π logger
    """
    # –°–æ–∑–¥–∞–µ–º formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, level.upper()))
    
    # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª)
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    
    # –ü–æ–¥–∞–≤–ª—è–µ–º –ª–∏—à–Ω–∏–µ warnings
    warnings.filterwarnings('ignore')
    
    return root_logger

def load_transaction_data(file_path: str, parse_dates: bool = True) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ CSV –∏ Parquet)
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        parse_dates: –ü–∞—Ä—Å–∏—Ç—å –ª–∏ –¥–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    
    Returns:
        DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"üìÅ Loading transaction data from {file_path}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not Path(file_path).exists():
            raise FileNotFoundError(f"Data file not found: {file_path}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        file_extension = Path(file_path).suffix.lower()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
        if file_extension == '.parquet':
            logger.info("üì¶ Loading Parquet file...")
            df = pd.read_parquet(file_path)
            # –î–ª—è parquet —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç—ã –æ–±—ã—á–Ω–æ —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
            if parse_dates and 'transaction_timestamp' in df.columns:
                if not pd.api.types.is_datetime64_any_dtype(df['transaction_timestamp']):
                    df['transaction_timestamp'] = pd.to_datetime(df['transaction_timestamp'])
        
        elif file_extension == '.csv':
            logger.info("üìÑ Loading CSV file...")
            if parse_dates:
                df = pd.read_csv(file_path, parse_dates=['transaction_timestamp'])
            else:
                df = pd.read_csv(file_path)
        
        else:
            raise ValueError(f"Unsupported file format: {file_extension}. Use .csv or .parquet")
        
        logger.info(f"‚úÖ Loaded {len(df):,} transactions for {df['card_id'].nunique():,} customers")
        
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Error loading data: {e}")
        raise

def validate_dataframe_schema(df: pd.DataFrame, expected_columns: List[str]) -> bool:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ö–µ–º—ã DataFrame
    
    Args:
        df: DataFrame –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        expected_columns: –û–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    
    Returns:
        True –µ—Å–ª–∏ —Å—Ö–µ–º–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"üîç Validating dataframe schema...")
    logger.info(f"üìä Dataset columns ({len(df.columns)}): {list(df.columns)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    missing_columns = set(expected_columns) - set(df.columns)
    if missing_columns:
        logger.warning(f"‚ö†Ô∏è Missing columns: {missing_columns}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    if 'transaction_timestamp' in df.columns:
        if not pd.api.types.is_datetime64_any_dtype(df['transaction_timestamp']):
            logger.warning("‚ö†Ô∏è transaction_timestamp is not datetime type")
            return False
    
    if 'card_id' in df.columns:
        if df['card_id'].isnull().any():
            logger.warning("‚ö†Ô∏è Found null values in card_id")
            return False
    
    if 'transaction_amount_kzt' in df.columns:
        if not pd.api.types.is_numeric_dtype(df['transaction_amount_kzt']):
            logger.warning("‚ö†Ô∏è transaction_amount_kzt is not numeric")
            return False
    
    logger.info(f"‚úÖ Schema validation passed - {len(expected_columns)} expected fields found")
    return True

def validate_features_dataframe(features_df: pd.DataFrame) -> bool:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è DataFrame —Å —Ñ–∏—á–∞–º–∏
    
    Args:
        features_df: DataFrame —Å feature engineering —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    
    Returns:
        True –µ—Å–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
    """
    logger = logging.getLogger(__name__)
    
    logger.info("üîç Validating features dataframe...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    required_cols = ['card_id']
    missing_required = set(required_cols) - set(features_df.columns)
    if missing_required:
        logger.error(f"‚ùå Missing required columns: {missing_required}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è
    nan_cols = features_df.columns[features_df.isnull().any()].tolist()
    if nan_cols:
        logger.warning(f"‚ö†Ô∏è Found NaN values in columns: {nan_cols}")
        logger.info("üí° NaN values will be filled with 0")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ infinite –∑–Ω–∞—á–µ–Ω–∏—è
    numeric_cols = features_df.select_dtypes(include=[np.number]).columns
    inf_cols = []
    for col in numeric_cols:
        if np.isinf(features_df[col]).any():
            inf_cols.append(col)
    
    if inf_cols:
        logger.warning(f"‚ö†Ô∏è Found infinite values in columns: {inf_cols}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    if len(features_df) == 0:
        logger.error("‚ùå Features dataframe is empty")
        return False
    
    logger.info(f"‚úÖ Features validation passed - {len(features_df)} customers, {len(features_df.columns)-1} features")
    return True

def clean_features_dataframe(features_df: pd.DataFrame) -> pd.DataFrame:
    """
    –û—á–∏—Å—Ç–∫–∞ DataFrame —Å —Ñ–∏—á–∞–º–∏
    
    Args:
        features_df: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
    """
    logger = logging.getLogger(__name__)
    
    logger.info("üßπ Cleaning features dataframe...")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é
    cleaned_df = features_df.copy()
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª—è–º–∏
    nan_count_before = cleaned_df.isnull().sum().sum()
    cleaned_df = cleaned_df.fillna(0)
    
    if nan_count_before > 0:
        logger.info(f"‚úÖ Filled {nan_count_before} NaN values with 0")
    
    # –ó–∞–º–µ–Ω—è–µ–º infinite –∑–Ω–∞—á–µ–Ω–∏—è
    numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns
    inf_count = 0
    
    for col in numeric_cols:
        inf_mask = np.isinf(cleaned_df[col])
        if inf_mask.any():
            inf_count += inf_mask.sum()
            # –ó–∞–º–µ–Ω—è–µ–º +inf –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            max_finite = cleaned_df[col][~inf_mask].max()
            cleaned_df.loc[inf_mask & (cleaned_df[col] > 0), col] = max_finite
            # –ó–∞–º–µ–Ω—è–µ–º -inf –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            min_finite = cleaned_df[col][~inf_mask].min()
            cleaned_df.loc[inf_mask & (cleaned_df[col] < 0), col] = min_finite
    
    if inf_count > 0:
        logger.info(f"‚úÖ Replaced {inf_count} infinite values")
    
    logger.info(f"‚úÖ Features cleaned - shape: {cleaned_df.shape}")
    return cleaned_df

def save_dataframe(df: pd.DataFrame, file_path: str, format: str = 'parquet') -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DataFrame –≤ —Ñ–∞–π–ª
    
    Args:
        df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ ('parquet', 'csv', 'json')
    
    Returns:
        True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"üíæ Saving dataframe to {file_path} ({format} format)...")
        
        if format.lower() == 'parquet':
            df.to_parquet(file_path, index=False)
        elif format.lower() == 'csv':
            df.to_csv(file_path, index=False)
        elif format.lower() == 'json':
            df.to_json(file_path, indent=2)
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        file_size = Path(file_path).stat().st_size / (1024 * 1024)  # MB
        logger.info(f"‚úÖ Saved successfully - {file_size:.2f} MB")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error saving dataframe: {e}")
        return False

def save_json(data: Dict[str, Any], file_path: str) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª
    
    Args:
        data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    
    Returns:
        True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    logger = logging.getLogger(__name__)
    
    try:
        import json
        
        logger.info(f"üíæ Saving JSON to {file_path}...")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"‚úÖ JSON saved successfully")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error saving JSON: {e}")
        return False

def print_dataframe_info(df: pd.DataFrame, name: str = "DataFrame") -> None:
    """
    –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ DataFrame
    
    Args:
        df: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        name: –ù–∞–∑–≤–∞–Ω–∏–µ DataFrame
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"üìä {name} Info:")
    logger.info(f"   Shape: {df.shape}")
    logger.info(f"   Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö
    dtype_counts = df.dtypes.value_counts()
    logger.info(f"   Data types: {dict(dtype_counts)}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ missing values
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        logger.info(f"   Missing values: {missing_count}")
    
    # –î—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        logger.info(f"   Duplicates: {duplicates}")

def set_random_state(seed: int = 42) -> None:
    """
    –§–∏–∫—Å–∞—Ü–∏—è random state –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Args:
        seed: –ó–Ω–∞—á–µ–Ω–∏–µ seed
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"üé≤ Setting random state to {seed} for reproducibility...")
    
    import random
    import numpy as np
    
    # Python random
    random.seed(seed)
    
    # NumPy random
    np.random.seed(seed)
    
    # –î–ª—è scikit-learn –±—É–¥–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å random_state –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    logger.info("‚úÖ Random state fixed for reproducibility")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
setup_logging() 