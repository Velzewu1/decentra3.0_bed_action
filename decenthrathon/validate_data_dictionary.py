#!/usr/bin/env python3
"""
–í–∞–ª–∏–¥–∞—Ü–∏—è Data Dictionary –ø—Ä–æ—Ç–∏–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
from core.data_processing import process_transaction_data

def validate_data_dictionary():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è data dictionary –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    print("üîç –í–ê–õ–ò–î–ê–¶–ò–Ø DATA DICTIONARY")
    print("="*50)
    
    # –ß–∏—Ç–∞–µ–º dictionary
    try:
        dict_df = pd.read_csv('data_dictionary.csv', comment='#')
        dict_fields = set(dict_df['–ù–∞–∑–≤–∞–Ω–∏–µ'].tolist())
        print(f"üìÑ –í data_dictionary.csv: {len(dict_fields)} –ø–æ–ª–µ–π")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è data_dictionary.csv: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    try:
        print("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±–æ—Ä–∫—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π...")
        df = pd.read_parquet('DECENTRATHON_3.0.parquet')
        df_sample = df.sample(1000, random_state=42)
        
        print("‚öôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ feature engineering...")
        features, _, _ = process_transaction_data(df_sample)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ segment (—Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏)
        engineered_fields = set(features.columns.tolist() + ['segment'])
        print(f"üéØ –ü–æ–ª–µ–π –ø–æ—Å–ª–µ feature engineering: {len(engineered_fields)}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return False
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —ç—Ç–∞–ø–∞–º
    print("\nüîç –ê–ù–ê–õ–ò–ó –ü–û –≠–¢–ê–ü–ê–ú –û–ë–†–ê–ë–û–¢–ö–ò:")
    print("-" * 40)
    
    # 1. –ò—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    original_fields = set(df.columns.tolist())
    dict_original = set(dict_df[dict_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] == '–ò—Å—Ö–æ–¥–Ω—ã–µ']['–ù–∞–∑–≤–∞–Ω–∏–µ'].tolist())
    
    print(f"1. –ò–°–•–û–î–ù–´–ï –¢–†–ê–ù–ó–ê–ö–¶–ò–ò:")
    print(f"   –í –ø–∞—Äquet —Ñ–∞–π–ª–µ: {len(original_fields)}")
    print(f"   –í —Å–ª–æ–≤–∞—Ä–µ: {len(dict_original)}")
    
    missing_original = original_fields - dict_original
    if missing_original:
        print(f"   ‚ö†Ô∏è  –ù–µ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã: {sorted(missing_original)}")
    
    extra_original = dict_original - original_fields
    if extra_original:
        print(f"   ‚ö†Ô∏è  –í —Å–ª–æ–≤–∞—Ä–µ –ª–∏—à–Ω–∏–µ: {sorted(extra_original)}")
    
    if not missing_original and not extra_original:
        print("   ‚úÖ –ò—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã")
    
    # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ feature engineering
    dict_engineered = set(dict_df[dict_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] != '–ò—Å—Ö–æ–¥–Ω—ã–µ']['–ù–∞–∑–≤–∞–Ω–∏–µ'].tolist())
    
    # card_id –æ—Å—Ç–∞–µ—Ç—Å—è –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    dict_engineered.add('card_id')
    
    print(f"\n2. –ü–û–°–õ–ï FEATURE ENGINEERING:")
    print(f"   –°–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ–ª–µ–π: {len(engineered_fields)}")
    print(f"   –í —Å–ª–æ–≤–∞—Ä–µ (–≤–∫–ª—é—á–∞—è card_id): {len(dict_engineered)}")
    
    missing_engineered = engineered_fields - dict_engineered
    if missing_engineered:
        print(f"   ‚ö†Ô∏è  –ù–µ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã: {sorted(missing_engineered)}")
    
    extra_engineered = dict_engineered - engineered_fields
    if extra_engineered:
        print(f"   ‚ö†Ô∏è  –í —Å–ª–æ–≤–∞—Ä–µ –ª–∏—à–Ω–∏–µ: {sorted(extra_engineered)}")
    
    if not missing_engineered and not extra_engineered:
        print("   ‚úÖ –°–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã")
    
    # 3. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    print(f"\n3. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
    categories = dict_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts()
    for cat, count in categories.items():
        print(f"   {cat}: {count} –ø–æ–ª–µ–π")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\nüéØ –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê:")
    print("-" * 20)
    
    total_issues = len(missing_original) + len(extra_original) + len(missing_engineered) + len(extra_engineered)
    
    if total_issues == 0:
        print("‚úÖ Data Dictionary –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω!")
        print("   - –í—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª—è –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã")
        print("   - –í—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã") 
        print("   - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return True
    else:
        print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {total_issues} –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π")
        if missing_original or extra_original:
            print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –ø–æ–ª—è–º–∏")
        if missing_engineered or extra_engineered:
            print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–∑–¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        return False

if __name__ == "__main__":
    validate_data_dictionary() 