#!/usr/bin/env python3
"""
Reports Module
Metrics + GPT-4 Insights + Export
"""

import pandas as pd
import numpy as np
import logging
import json
from typing import Dict, Any, Optional
from datetime import datetime

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

from config import get_config
from utils.helpers import save_dataframe, save_json

logger = logging.getLogger(__name__)

class MetricsCalculator:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.config = get_config()
    
    def calculate_all_metrics(self, data: np.ndarray, labels: np.ndarray, algorithm_name: str) -> Dict[str, Any]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            labels: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            algorithm_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        logger.info(f"üìä Calculating quality metrics for {algorithm_name}...")
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        noise_ratio = np.sum(labels == -1) / len(labels)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º noise —Ç–æ—á–∫–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        non_noise = labels != -1
        
        metrics = {
            'algorithm': algorithm_name,
            'n_clusters': n_clusters,
            'noise_ratio': noise_ratio,
            'total_points': len(labels),
            'non_noise_points': np.sum(non_noise)
        }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ-noise —Ç–æ—á–µ–∫
        if np.sum(non_noise) > 1 and n_clusters > 1:
            try:
                # Silhouette Score ([-1, 1], –±–æ–ª—å—à–µ = –ª—É—á—à–µ)
                silhouette = silhouette_score(data[non_noise], labels[non_noise])
                metrics['silhouette_score'] = silhouette
                
                # Davies-Bouldin Index ([0, ‚àû), –º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
                davies_bouldin = davies_bouldin_score(data[non_noise], labels[non_noise])
                metrics['davies_bouldin'] = davies_bouldin
                
                # Calinski-Harabasz Index ([0, ‚àû), –±–æ–ª—å—à–µ = –ª—É—á—à–µ)
                calinski_harabasz = calinski_harabasz_score(data[non_noise], labels[non_noise])
                metrics['calinski_harabasz'] = calinski_harabasz
                
                logger.info(f"‚úÖ Quality metrics calculated:")
                logger.info(f"   üéØ Clusters: {n_clusters}")
                logger.info(f"   üîá Noise: {noise_ratio:.1%}")
                logger.info(f"   üìà Silhouette: {silhouette:.3f}")
                logger.info(f"   üìä Davies-Bouldin: {davies_bouldin:.3f}")
                logger.info(f"   ‚ö° Calinski-Harabasz: {calinski_harabasz:.1f}")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not calculate some metrics: {e}")
                metrics.update({
                    'silhouette_score': None,
                    'davies_bouldin': None,
                    'calinski_harabasz': None
                })
        else:
            logger.warning("‚ö†Ô∏è Insufficient data for quality metrics")
            metrics.update({
                'silhouette_score': None,
                'davies_bouldin': None,
                'calinski_harabasz': None
            })
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        if n_clusters > 0:
            cluster_sizes = []
            for cluster_id in set(labels):
                if cluster_id != -1:
                    cluster_sizes.append(np.sum(labels == cluster_id))
            
            if cluster_sizes:
                metrics['min_cluster_size'] = min(cluster_sizes)
                metrics['max_cluster_size'] = max(cluster_sizes)
                metrics['avg_cluster_size'] = np.mean(cluster_sizes)
                metrics['cluster_balance'] = 1.0 - (np.std(cluster_sizes) / np.mean(cluster_sizes)) if len(cluster_sizes) > 1 else 1.0
        
        return metrics


class GPTInsightsGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4"""
    
    def __init__(self):
        self.config = get_config()
        self.api_key = self.config.OPENAI_API_KEY
        
    def generate_insights(self, features_df: pd.DataFrame, labels: np.ndarray, 
                         algorithm_name: str, quality_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4
        
        Args:
            features_df: DataFrame —Å —Ñ–∏—á–∞–º–∏
            labels: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            algorithm_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
            quality_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Å–∞–π—Ç–∞–º–∏
        """
        logger.info("üß† GENERATING GPT-4 INSIGHTS...")
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            segment_stats = self._prepare_segment_statistics(features_df, labels)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º prompt –¥–ª—è GPT-4
            prompt = self._create_gpt_prompt(segment_stats, algorithm_name, quality_metrics)
            
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT-4
            response = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are an expert banking analyst specializing in customer segmentation. Return only valid JSON without markdown formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            insights = self._process_gpt_response(response)
            
            logger.info("‚úÖ GPT-4 insights generated successfully")
            return insights
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è GPT-4 integration failed: {e}")
            logger.info("üí° Using fallback segment analysis...")
            
            # Fallback –∞–Ω–∞–ª–∏–∑
            return self._generate_fallback_insights(features_df, labels)
    
    def _prepare_segment_statistics(self, features_df: pd.DataFrame, labels: np.ndarray) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º"""
        features_with_labels = features_df.copy()
        features_with_labels['segment'] = labels
        
        segment_stats = {}
        for segment_id in sorted(set(labels)):
            if segment_id == -1:
                segment_name = "Outliers"
            else:
                segment_name = f"Segment_{segment_id}"
            
            segment_data = features_with_labels[features_with_labels['segment'] == segment_id]
            
            segment_stats[segment_name] = {
                'size': len(segment_data),
                'percentage': len(segment_data) / len(features_with_labels) * 100,
                'avg_transactions': float(segment_data['tx_count'].mean()),
                'avg_amount': float(segment_data['avg_amount'].mean()),
                'total_revenue': float(segment_data['total_amount'].sum()),
                'digital_wallet_ratio': float(segment_data['digital_wallet_ratio'].mean()),
                'international_ratio': float(segment_data['international_ratio'].mean()),
                'contactless_ratio': float(segment_data['contactless_ratio'].mean()),
                'weekend_ratio': float(segment_data['weekend_ratio'].mean()),
                'city_diversity': float(segment_data['city_diversity'].mean()),
                'country_diversity': float(segment_data['country_diversity'].mean()),
                'payment_sophistication': float(segment_data['payment_sophistication'].mean())
            }
        
        return segment_stats
    
    def _create_gpt_prompt(self, segment_stats: Dict[str, Any], algorithm_name: str, 
                          quality_metrics: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ prompt –¥–ª—è GPT-4"""
        return f"""
You are a senior banking analyst. Analyze these customer segments and provide:

1. MEANINGFUL NAMES for each segment based on behavior
2. KEY CHARACTERISTICS of each segment  
3. BUSINESS RECOMMENDATIONS (marketing, products, retention)
4. REVENUE OPPORTUNITIES for each segment

Algorithm Used: {algorithm_name}
Quality Metrics: {quality_metrics}

Segment Statistics:
{json.dumps(segment_stats, indent=2, default=str)}

Return structured JSON with format:
{{
    "segment_name": {{
        "business_name": "Meaningful name",
        "description": "Key characteristics", 
        "recommendations": ["action1", "action2", "action3"],
        "revenue_opportunity": "How to monetize"
    }}
}}
"""
    
    def _process_gpt_response(self, response) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT-4"""
        response_content = response.choices[0].message.content.strip()
        
        # –£–¥–∞–ª—è–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if response_content.startswith('```json'):
            response_content = response_content[7:]
        if response_content.startswith('```'):
            response_content = response_content[3:]
        if response_content.endswith('```'):
            response_content = response_content[:-3]
        
        response_content = response_content.strip()
        
        logger.info(f"üîç Cleaned response length: {len(response_content)} chars")
        
        insights = json.loads(response_content)
        return insights
    
    def _generate_fallback_insights(self, features_df: pd.DataFrame, labels: np.ndarray) -> Dict[str, Any]:
        """Fallback –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ GPT-4"""
        segment_stats = self._prepare_segment_statistics(features_df, labels)
        
        fallback_insights = {}
        for segment_name, stats in segment_stats.items():
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            if segment_name == "Outliers":
                business_name = "High-Value Outliers"
                description = "Customers with unusual transaction patterns requiring individual attention"
                recommendations = ["Individual analysis", "Fraud monitoring", "Premium service"]
            elif stats['avg_amount'] > 20000:
                business_name = "Premium Customers"
                description = "High-value customers with large transaction amounts"
                recommendations = ["VIP service", "Premium products", "Exclusive offers"]
            elif stats['digital_wallet_ratio'] > 0.6:
                business_name = "Digital Natives"
                description = "Tech-savvy customers preferring digital payments"
                recommendations = ["Mobile banking features", "Digital-first products", "App promotions"]
            elif stats['avg_transactions'] > 1000:
                business_name = "Active Users"
                description = "Highly engaged customers with frequent transactions"
                recommendations = ["Loyalty programs", "Transaction bonuses", "Cashback offers"]
            else:
                business_name = "Standard Customers"
                description = "Regular banking customers with typical patterns"
                recommendations = ["Standard products", "Education campaigns", "Engagement programs"]
            
            fallback_insights[segment_name] = {
                "business_name": business_name,
                "description": description,
                "recommendations": recommendations,
                "revenue_opportunity": "Targeted banking products and services"
            }
        
        return fallback_insights


class ReportExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
    
    def __init__(self):
        self.config = get_config()
        self.paths = {
            'segments_parquet': self.config.SEGMENTS_FILE,
            'segments_csv': self.config.SEGMENTS_CSV,
            'summary_parquet': self.config.SUMMARY_FILE,
            'results_json': self.config.RESULTS_JSON,
            'insights_json': self.config.INSIGHTS_JSON
        }
    
    def export_all_results(self, features_df: pd.DataFrame, labels: np.ndarray,
                          quality_metrics: Dict[str, Any], insights: Dict[str, Any],
                          algorithm_results: Dict[str, Any], algorithm_name: str) -> bool:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            features_df: DataFrame —Å —Ñ–∏—á–∞–º–∏
            labels: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            quality_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            insights: GPT-4 –∏–Ω—Å–∞–π—Ç—ã
            algorithm_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
            algorithm_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        
        Returns:
            True –µ—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω
        """
        logger.info("üíæ EXPORTING RESULTS...")
        
        try:
            # 1. –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
            self._export_main_results(features_df, labels)
            
            # 2. –≠–∫—Å–ø–æ—Ä—Ç —Å–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
            self._export_segment_summary(features_df, labels)
            
            # 3. –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self._export_technical_results(quality_metrics, algorithm_results, algorithm_name)
            
            # 4. –≠–∫—Å–ø–æ—Ä—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤
            self._export_business_insights(insights)
            
            logger.info("‚úÖ All results exported successfully")
            logger.info(f"üìÅ Files created:")
            for name, path in self.paths.items():
                logger.info(f"   üìã {path}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Export failed: {e}")
            return False
    
    def _export_main_results(self, features_df: pd.DataFrame, labels: np.ndarray) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏"""
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –∫ —Ñ–∏—á–∞–º
        results_df = features_df.copy()
        results_df['segment'] = labels
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        save_dataframe(results_df, self.paths['segments_parquet'], format='parquet')
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        save_dataframe(results_df, self.paths['segments_csv'], format='csv')
    
    def _export_segment_summary(self, features_df: pd.DataFrame, labels: np.ndarray) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç —Å–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º"""
        features_with_labels = features_df.copy()
        features_with_labels['segment'] = labels
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        summary_data = []
        for segment_id in sorted(set(labels)):
            segment_data = features_with_labels[features_with_labels['segment'] == segment_id]
            
            summary = {
                'segment': segment_id,
                'segment_name': 'Outliers' if segment_id == -1 else f'Segment_{segment_id}',
                'size': len(segment_data),
                'percentage': len(segment_data) / len(features_with_labels) * 100,
                'avg_amount': segment_data['avg_amount'].mean(),
                'total_revenue': segment_data['total_amount'].sum(),
                'avg_transactions': segment_data['tx_count'].mean(),
                'digital_wallet_ratio': segment_data['digital_wallet_ratio'].mean(),
                'contactless_ratio': segment_data['contactless_ratio'].mean(),
                'international_ratio': segment_data['international_ratio'].mean(),
                'city_diversity': segment_data['city_diversity'].mean(),
                'payment_sophistication': segment_data['payment_sophistication'].mean()
            }
            summary_data.append(summary)
        
        summary_df = pd.DataFrame(summary_data)
        save_dataframe(summary_df, self.paths['summary_parquet'], format='parquet')
    
    def _export_technical_results(self, quality_metrics: Dict[str, Any], 
                                 algorithm_results: Dict[str, Any], algorithm_name: str) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        technical_results = {
            'timestamp': datetime.now().isoformat(),
            'algorithm': algorithm_name,
            'quality_metrics': quality_metrics,
            'algorithm_results': {k: v for k, v in algorithm_results.items() if k != 'clusterer'},  # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—ä–µ–∫—Ç
            'config': {
                'clustering_params': self.config.CLUSTERING_PARAMS,
                'preprocessing_params': self.config.PREPROCESSING,
                'optimization_params': self.config.OPTIMIZATION
            }
        }
        
        save_json(technical_results, self.paths['results_json'])
    
    def _export_business_insights(self, insights: Dict[str, Any]) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤"""
        business_insights = {
            'timestamp': datetime.now().isoformat(),
            'insights': insights,
            'summary': self._create_executive_summary(insights)
        }
        
        save_json(business_insights, self.paths['insights_json'])
    
    def _create_executive_summary(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ executive summary"""
        total_segments = len(insights)
        segment_names = [insight.get('business_name', 'Unknown') for insight in insights.values()]
        
        return {
            'total_segments': total_segments,
            'segment_names': segment_names,
            'key_opportunities': [
                "Digital transformation acceleration",
                "Premium customer retention",
                "Cross-selling optimization",
                "Risk management enhancement"
            ],
            'expected_impact': {
                'revenue_growth': "15-20%",
                'customer_retention': "+10%",
                'cross_sell_rate': "+25%"
            }
        }


def generate_comprehensive_report(features_df: pd.DataFrame, labels: np.ndarray, 
                                ml_features_processed: np.ndarray, algorithm_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    
    Args:
        features_df: DataFrame —Å —Ñ–∏—á–∞–º–∏
        labels: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        ml_features_processed: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        algorithm_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    
    Returns:
        –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏
    """
    logger.info("üìä GENERATING COMPREHENSIVE REPORT...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–µ
    algorithm_name = list(algorithm_results.keys())[0]
    algorithm_data = algorithm_results[algorithm_name]
    
    # 1. –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    metrics_calc = MetricsCalculator()
    quality_metrics = metrics_calc.calculate_all_metrics(
        ml_features_processed, labels, algorithm_data['algorithm_name']
    )
    
    # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º GPT-4 –∏–Ω—Å–∞–π—Ç—ã
    insights_gen = GPTInsightsGenerator()
    insights = insights_gen.generate_insights(
        features_df, labels, algorithm_data['algorithm_name'], quality_metrics
    )
    
    # 3. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    exporter = ReportExporter()
    export_success = exporter.export_all_results(
        features_df, labels, quality_metrics, insights, algorithm_results, algorithm_name
    )
    
    # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    final_report = {
        'algorithm': algorithm_data['algorithm_name'],
        'quality_metrics': quality_metrics,
        'insights': insights,
        'export_success': export_success,
        'files_created': exporter.paths if export_success else {}
    }
    
    logger.info("‚úÖ Comprehensive report generated")
    
    return final_report 