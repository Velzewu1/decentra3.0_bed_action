#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
"""

import time
import os
from core.data_processing import process_transaction_data
from core.clustering import perform_clustering
from utils.helpers import load_transaction_data, setup_logging, save_dataframe
from reporting.reports import generate_comprehensive_report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    print("üöÄ REFACTORED CUSTOMER SEGMENTATION PIPELINE")
    print("="*60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging('INFO')
    logger.info("üéØ Starting balanced clustering pipeline...")
    
    # –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
    start_time = time.time()
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìÇ –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    df = load_transaction_data('transactions.csv')
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {len(df):,}")
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüîß –≠–¢–ê–ü 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    features_df, ml_features_processed, preprocessing_pipeline = process_transaction_data(df)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {ml_features_processed.shape[1]}")
    print(f"‚úÖ –ö–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(features_df):,}")
    
    # 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
    print("\nüéØ –≠–¢–ê–ü 3: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    print("-" * 40)
    
    from config import get_config
    config = get_config()
    
    labels, clusterer, clustering_results = perform_clustering(
        ml_features_processed, 
        config.CLUSTERING_PARAMS
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    features_df['segment'] = labels
    
    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
    print("\nüìä –≠–¢–ê–ü 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤")
    print("-" * 40)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º results –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
    algorithm_name = clustering_results.get('gmm_best', clustering_results.get('kmeans_best', clustering_results.get('hdbscan_best', {})))
    algorithm_key = f"{algorithm_name.get('params', {}).get('algorithm', 'unknown')}_balanced"
    
    formatted_results = {
        algorithm_key: {
            'labels': labels,
            'clusterer': clusterer,
            'params': algorithm_name.get('params', {}),
            'metrics': algorithm_name.get('metrics', {}),
            'algorithm_name': f"Balanced {algorithm_name.get('params', {}).get('algorithm', 'Unknown').upper()}"
        }
    }
    
    final_report = generate_comprehensive_report(
        features_df, labels, ml_features_processed, formatted_results
    )
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüíæ –≠–¢–ê–ü 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("-" * 40)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
    output_files = [
        'customer_segments_refactored.csv',
        'customer_segments_refactored.parquet'
    ]
    
    for filename in output_files:
        save_dataframe(features_df, filename)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    total_time = time.time() - start_time
    print(f"\n‚è±Ô∏è  –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    print(f"\nüéâ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print(f"üìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(set(labels))}")
    print(f"üë• –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(features_df):,}")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.getcwd()}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    import numpy as np
    unique_labels = np.unique(labels)
    cluster_sizes = [np.sum(labels == label) for label in unique_labels]
    max_size = max(cluster_sizes)
    min_size = min(cluster_sizes)
    balance_ratio = min_size / max_size
    largest_pct = max_size / len(labels) * 100
    
    print(f"\nüìà –ö–ê–ß–ï–°–¢–í–û –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò:")
    print(f"   –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {sorted(cluster_sizes, reverse=True)}")
    print(f"   –ë–∞–ª–∞–Ω—Å (–º–∏–Ω/–º–∞–∫—Å): {balance_ratio:.3f}")
    print(f"   –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä: {largest_pct:.1f}%")
    
    if largest_pct < 50:
        print(f"   ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å!")
    elif largest_pct < 70:
        print(f"   ‚ö†Ô∏è  –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")
    else:
        print(f"   ‚ùå –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏")

if __name__ == "__main__":
    main() 